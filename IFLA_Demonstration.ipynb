{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e517d8",
   "metadata": {},
   "source": [
    "<h1>IFLA WLIC - 2022</h1>\n",
    "<h2><span style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">From text to data inside bibliographic records. Entity recognition and entity linking of contributors and their roles from statements of responsibility</span></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f0729",
   "metadata": {},
   "source": [
    "<h2>Imports - Necessary libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20588c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formats\n",
    "import csv\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "\n",
    "# Functions\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "#Visualisation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Data pre-treatment\n",
    "from texthero import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import texthero as hero\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71e0f0",
   "metadata": {},
   "source": [
    "<h2>Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_match(name1,name2):\n",
    "    titreCivilite=0\n",
    "    score=0\n",
    "    name1=name1.replace(\"-\",\" \")\n",
    "    name1 = re.sub(r\"(.[.])()(.[.])\",r\"\\1 \\3\", name1)\n",
    "    name2=name2.replace(\"-\",\" \")\n",
    "    name_components_1 = name1.split(\" \")\n",
    "    name_components_2 = name2.split(\" \")\n",
    "    nameComponent1Match={}\n",
    "    nameComponent2Match={}\n",
    "    for index in range(len(name_components_1)):\n",
    "        name_component1=unidecode.unidecode(name_components_1[index]).lower()\n",
    "        ##TITLE##\n",
    "        if(index==0 and (name_component1 in [\"m.\",\"mme.\",\"dr.\"])):\n",
    "            titreCivilite=1\n",
    "            score+=1\n",
    "        ##ABREVIATIION##\n",
    "        elif(len(name_component1)<3 or (\".\" in name_component1)):\n",
    "            for index2 in range(len(name_components_2)):\n",
    "                name_component2=unidecode.unidecode(name_components_2[index2]).lower()\n",
    "                if(name_component2!=''):\n",
    "                    match=True\n",
    "                    for i in range(0,len(name_component1)):\n",
    "                        if(i<len(name_component2)and name_component1[i]!=name_component2[i] and name_component1[i]!=\".\"):\n",
    "                            match=False\n",
    "                    if(match):\n",
    "                        if(name_component1 not in nameComponent1Match.keys()):\n",
    "                            nameComponent1Match[name_component1]=1\n",
    "                        else:\n",
    "                            nameComponent1Match[name_component1]+=1\n",
    "                        if(name_component2 not in nameComponent2Match.keys()):\n",
    "                            nameComponent2Match[name_component2]=1\n",
    "                        else:\n",
    "                            nameComponent2Match[name_component2]+=1\n",
    "        ##Complete Name##\n",
    "        else:\n",
    "            for index2 in range(len(name_components_2)):\n",
    "                    name_component2=unidecode.unidecode(name_components_2[index2]).lower()\n",
    "                    ratio = fuzz.ratio(name_component1,name_component2)\n",
    "                    if(name_component2!=''):\n",
    "                        if(name_component1==name_component2 or (ratio>=80)):\n",
    "                            if(name_component1 not in nameComponent1Match.keys()):\n",
    "                                alignment=''\n",
    "                                for key in nameComponent1Match.keys():\n",
    "                                    if(fuzz.ratio(name_component1,key)>=80):\n",
    "                                        alignment=key\n",
    "                                if(alignment==''):\n",
    "                                    nameComponent1Match[name_component1]=1\n",
    "                                else:\n",
    "                                    nameComponent1Match[alignment]+=1\n",
    "                            else:\n",
    "                                nameComponent1Match[name_component1]+=1\n",
    "                            if(name_component2 not in nameComponent2Match.keys()):\n",
    "                                alignment=''\n",
    "                                for key in nameComponent2Match.keys():\n",
    "                                    if(fuzz.ratio(name_component2,key)>=80):\n",
    "                                        alignment=key\n",
    "                                if(alignment==''):\n",
    "                                    nameComponent2Match[name_component2]=1\n",
    "                                else:\n",
    "                                    nameComponent2Match[alignment]+=1\n",
    "                            else:\n",
    "                                nameComponent2Match[name_component2]+=1                        \n",
    "    for key in nameComponent1Match.keys():\n",
    "        score+=nameComponent1Match[key]    \n",
    "    \n",
    "    #print(\"Matching between \"+name1+\" and \"+name2+\" has a score of \"+str(score))\n",
    "    if(score==len(name_components_1) and (len(nameComponent2Match.keys())+titreCivilite)==(len(name_components_1))):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def create_trainingSet(load_name,save_name):\n",
    "    model = spacy.load(\"./Model_NER_PERS_FONCT_10K\")\n",
    "    rows=[]\n",
    "    file = open(\"./data/\"+load_name,mode='r',encoding=\"utf8\")\n",
    "    csvreader = csv.reader(file, delimiter=\",\")\n",
    "    header = next(csvreader)\n",
    "    dicoLabelAmt={}\n",
    "    #############For ever bibliographical record entry in the csv file#############\n",
    "    for row in csvreader:\n",
    "        ppn=row[1]\n",
    "        bibliographicalRecord = row[2]\n",
    "        accessPoints = row[3]\n",
    "        documentSupports = row[4]\n",
    "        ppnLinks = row[5]\n",
    "        textualRessources = row[6]\n",
    "        mentions = bibliographicalRecord.split(\" / \",1)\n",
    "        ###############################Creating table of supports#############################\n",
    "        supports = documentSupports.split(\",\")\n",
    "        ######################################################################################\n",
    "        ##########################Creating access points dictionary##########################\n",
    "        dictionnaireAccessPoints = {}\n",
    "        codes=[]\n",
    "        accessPoint = accessPoints.split(\" \")\n",
    "        name=\"\"\n",
    "        for token in accessPoint :\n",
    "            if(token.isnumeric() and len(name)==0):\n",
    "                codes.append(token)\n",
    "            elif(token.isnumeric()):\n",
    "                #Checking the person is associate to only a single UNIMARC code function to avoi ambiguity\n",
    "                if (len(name)>0 and len(codes)==1):\n",
    "                    dictionnaireAccessPoints[name]=codes\n",
    "                codes=[]\n",
    "                name=\"\"\n",
    "                codes.append(token)\n",
    "            elif(len(name)==0):\n",
    "                name+=token\n",
    "            else :\n",
    "                name+=\" \"\n",
    "                name+=token\n",
    "        #Checking the person is associate to only a single UNIMARC code function to avoi ambiguity\n",
    "        if (len(name)>0 and len(codes)==1):\n",
    "            dictionnaireAccessPoints[name]=codes\n",
    "        ########################################################################################\n",
    "        ###################################Feature 'Thesis'#####################################\n",
    "        isThesis = False\n",
    "        for link in ppnLinks.split(','):\n",
    "            if(link==\"027253139\"):\n",
    "                isThesis=True\n",
    "        for ressource in textualRessources.split(','):\n",
    "            if(('m' in ressource[4:8])or('v' in ressource[4:8])):\n",
    "                isThesis=True\n",
    "        ########################################################################################\n",
    "        ####################################Extracting LABEL####################################\n",
    "        if(len(mentions)>=2):\n",
    "            ###Position of the mention###\n",
    "            position=0\n",
    "            ###Iterating through the mention of responsibilities###\n",
    "            for mention in mentions[1].split(\" ; \"):\n",
    "                dictPersonFunct={}\n",
    "                doc = model(mention)\n",
    "                ###Pairing function keywords to their appropriate persons###\n",
    "                persons=[]\n",
    "                keywords=[]\n",
    "                currentFunct=''\n",
    "                switch=0\n",
    "                for ent in doc.ents:\n",
    "                    if(currentFunct==\"\"):\n",
    "                        currentFunct=ent.label_\n",
    "                    if(currentFunct!=ent.label_):\n",
    "                        currentFunct=ent.label_\n",
    "                        switch+=1\n",
    "                    if(switch%2==0 and switch>0):\n",
    "                        for person in persons:\n",
    "                            if(person in dictPersonFunct.keys()):\n",
    "                                dictPersonFunct[person].append(keywords)\n",
    "                            else:\n",
    "                                dictPersonFunct[person]=keywords\n",
    "                        persons=[]\n",
    "                        keywords=[]\n",
    "                    if(ent.label_== \"PER\"):\n",
    "                        persons.append(ent)\n",
    "                    if(ent.label_ == \"FONCT\"):\n",
    "                        keywords.append(ent)\n",
    "                for person in persons:\n",
    "                    if(person in dictPersonFunct.keys()):\n",
    "                        dictPersonFunct[person].append(keywords)\n",
    "                    else:\n",
    "                        dictPersonFunct[person]=keywords\n",
    "                ############################################################\n",
    "                for person in dictPersonFunct.keys():\n",
    "                    for key in dictionnaireAccessPoints.keys():\n",
    "                        if(name_match(person.text, key)):\n",
    "                            ###Counting amount of entries per function###\n",
    "                            if(dictionnaireAccessPoints[key][0] in dicoLabelAmt.keys()):\n",
    "                                dicoLabelAmt[dictionnaireAccessPoints[key][0]]+=1\n",
    "                            else:\n",
    "                                dicoLabelAmt[dictionnaireAccessPoints[key][0]]=1\n",
    "                            ############################################\n",
    "                            rows.append([ppn,person.text,dictPersonFunct[person],position,supports,isThesis,dictionnaireAccessPoints[key][0]])\n",
    "                ###incrementing position of the mention###\n",
    "                position+=1\n",
    "    ########################################################################################\n",
    "    ##################################Writing training dataset##############################\n",
    "    file2 = open(\"./data/TRAINING_PPN_PERSON_Keywords_Position_DocSupport_Thesis\"+save_name+\".csv\",mode='w',encoding=\"utf8\",newline='')\n",
    "    writer = csv.writer(file2)\n",
    "    writer.writerow([\"PPN\",\"Person\",\"Keywords\",\"Position\",\"Document Support\",\"Thesis\",\"Label\"])\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "    print(\"There are \"+str(len(rows))+\" entries\")\n",
    "    print(\"Final count of training entries per function :\")\n",
    "    for key in dicoLabelAmt.keys():\n",
    "        print(\"Function \"+str(key)+\" has \"+str(dicoLabelAmt[key])+\" entries.\")\n",
    "        \n",
    "def encodage_données(dataframe,featuresBinaires, featuresOrdinaux, featuresNominaux, featuresTextes):\n",
    "    encodedDataframe = dataframe.copy()\n",
    "    ###Relocating target##\n",
    "    labelColumn = encodedDataframe[\"Label\"] \n",
    "    del encodedDataframe[\"Label\"]\n",
    "    binaryEncoders=[]\n",
    "    ordinalEncoders=[]\n",
    "    nominalEncoders=[]\n",
    "    textEncoders=[]\n",
    "    #####\n",
    "    clean_pipeline=[preprocessing.lowercase,preprocessing.remove_whitespace,preprocessing.remove_whitespace,preprocessing.remove_square_brackets]\n",
    "    #####\n",
    "    ###Categorical Features###\n",
    "    for binaryFeature in featuresBinaires:\n",
    "        print(\"Encoding Binary Feature : \"+str(binaryFeature))\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(encodedDataframe[binaryFeature])\n",
    "        encodedDataframe[binaryFeature]=label_encoder.transform(dataframe[binaryFeature])\n",
    "        binaryEncoders.append(label_encoder)\n",
    "    for ordinalFeature in featuresOrdinaux:\n",
    "        pass\n",
    "        #IMPLEMENT\n",
    "    for nominalFeature in featuresNominaux:\n",
    "        print(\"Encoding Nominal Feature : \"+str(nominalFeature))\n",
    "        encodedDataframe = pd.get_dummies(encodedDataframe, prefix = [nominalFeature], columns = [nominalFeature])\n",
    "        nominalEncoders.append(nominalFeature)\n",
    "    ###Text Features###\n",
    "    for textFeature in featuresTextes:\n",
    "        print(\"Encoding Text Feature : \"+str(textFeature))\n",
    "        #######CLEAN UP#########\n",
    "        encodedDataframe[textFeature] = [n.replace('[','') for n in encodedDataframe[textFeature]]\n",
    "        encodedDataframe[textFeature] = [n.replace(']','') for n in encodedDataframe[textFeature]]\n",
    "        encodedDataframe[textFeature] = hero.clean(encodedDataframe[textFeature],clean_pipeline)\n",
    "        ########################\n",
    "        ###Creating TF-IDF df###\n",
    "        encodedDataframe[textFeature]=hero.tfidf(encodedDataframe[textFeature], max_features=3000)\n",
    "        allwordsAmt=len(encodedDataframe[textFeature][0])\n",
    "        lsts=list(encodedDataframe[textFeature])\n",
    "        column_names=[]\n",
    "        for i in range(0,allwordsAmt):\n",
    "            column_names.append(str(textFeature)+\"_\"+str(i))\n",
    "        dataframeTmp = pd.DataFrame(lsts, columns =column_names, dtype = float)\n",
    "        ########################\n",
    "        del encodedDataframe[textFeature]\n",
    "        encodedDataframe = pd.concat([encodedDataframe.reset_index(drop=True),dataframeTmp.reset_index(drop=True)], axis=1)\n",
    "    ###Relocating target###\n",
    "    #encodedDataframe[\"Label\"]=labelColumn\n",
    "    #######################\n",
    "    return encodedDataframe,labelColumn,binaryEncoders, ordinalEncoders, nominalEncoders, textEncoders\n",
    "\n",
    "def Matrice_Confusion(Y_test,Y_pred,name):\n",
    "    ### Confusion Matrix\n",
    "    cm = confusion_matrix(Y_test,Y_pred)\n",
    "    ## Get Class Labels\n",
    "    labels=[]\n",
    "    for string in Y_test:\n",
    "        if(string not in labels):\n",
    "            labels.append(string)\n",
    "    labels.sort()\n",
    "    print(labels)\n",
    "    print(len(labels))\n",
    "    class_names = labels\n",
    "    # Plot confusion matrix in a beautiful manner\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted', fontsize=20)\n",
    "    ax.xaxis.set_label_position('bottom')\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(class_names, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(class_names, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    plt.title('Matrice de Confusion', fontsize=20)\n",
    "    \n",
    "    pyplot.savefig(\"./confusion_\"+name+\".png\")\n",
    "    \n",
    "    plt.close('all')\n",
    "    \n",
    "def visualisation_erreurs(df,X_test_PPN,Y_pred,Y_test,name):\n",
    "    Label_compare=Y_test.to_frame()\n",
    "    Label_compare[\"Prediction\"]=Y_pred\n",
    "    Label_compare[\"PPN\"]=X_test_PPN[\"PPN\"]\n",
    "    Label_compare[\"Personne\"]=X_test_PPN[\"Personne\"]\n",
    "    fichier2 = open(\"./data/csv/erreurs_\"+name+\".csv\", 'w')\n",
    "    writer = csv.writer(fichier2)\n",
    "    writer.writerow([\"PPN\",\"Personne\",\"Keywords\",\"Position\",\"Type\",\"These\",\"Predicted_Label\",\"True_Label\"])\n",
    "    for ind in Label_compare.index:\n",
    "        if(Label_compare['Label'][ind]!=Label_compare['Prediction'][ind]):\n",
    "            writer.writerow([Label_compare['PPN'][ind],Label_compare['Personne'][ind],df['Position'][ind],df['Keywords'][ind],df['Type'][ind],df['These'][ind],Label_compare['Prediction'][ind],Label_compare['Label'][ind]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff925c0",
   "metadata": {},
   "source": [
    "<h2>Demonstration</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72556a61",
   "metadata": {},
   "source": [
    "<h3>NER (Named Entity Recognition) Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fab18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load(\"./Model_NER_PERS_FONCT_10K\")\n",
    "doc = model(\"écrit par Zac Thompson et Emily Horn, dessiné par Alberto Albuquerque\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5047e17",
   "metadata": {},
   "source": [
    "<h3>Creating training sets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b484f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_name1=[\"F. Nietzsche\",\"Alberto Albuquerque\",\"M. Zaragoza\",\"Mme. Zaragoza\",\"Thomâs Zargoza\",\"Mircel Frontignan\",\"J-C Van Damme\",\"Th Maximilien\",\"J.L. Istin\"]\n",
    "true_name2=[\"Friedrich Nietzsche\",\"Alberto Jiménez Albuquerque\",\"Thomas Zaragoza\",\"Zaragoza Thomas\",\"Thomas Zaragoza\",\"Marcel Frontagnan\",\"Jean-Claude Van Damme\",\"Théophile Maximilien\",\"Jean-Luc Istin\"]\n",
    "false_name1=[\"G. Nietzsche\",\"Frank Nietzsche\",\"Z. Zaragoza\",\"Marcel Frontignan\"]\n",
    "false_name2=[\"Friedrich Nietzsche\",\"Friedrich Nietzsche\",\"Thomas Zaragoza\",\"Marco Frontignan\"]\n",
    "print(\"Matching between these should be accepted\")\n",
    "print(\" \")\n",
    "for x in range(len(true_name1)):\n",
    "    print(true_name1[x]+\" and \"+true_name2[x]+\" are matching: \"+str(name_match(true_name1[x],true_name2[x])))\n",
    "    Token_Set_Ratio = fuzz.token_set_ratio(true_name1[x],true_name2[x])\n",
    "    print(\"Token_Set_Ratio is :\"+str(Token_Set_Ratio))\n",
    "print(\" \")\n",
    "print(\"========================================\")\n",
    "print(\" \")\n",
    "print(\"Matching between these should be refused\")\n",
    "print(\" \")\n",
    "for x in range(len(false_name1)):\n",
    "    print(false_name1[x]+\" and \"+false_name2[x]+\" are matching : \"+str(name_match(false_name1[x],false_name2[x])))\n",
    "    Token_Set_Ratio = fuzz.token_set_ratio(false_name1[x],false_name2[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52649c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_trainingSet(\"ID_PPN_Mentions_AP_DocSup_PPNLink_TextualRessources_TEST.csv\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ef8ed",
   "metadata": {},
   "source": [
    "<h2>Loading and encoding training set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa387bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/TRAINING_Classes_13_Sample_1000.csv\")\n",
    "encodedDataframe,targetDataframe,binaryEncoders, ordinalEncoders, nominalEncoders, textEncoders = encodage_données(df,[\"Thesis\"],[],[],[\"Document support\",\"Keywords\"])\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(encodedDataframe,targetDataframe,test_size=0.2,random_state=0)\n",
    "X_train_PPN_Personne = X_train[['PPN','Personne']]\n",
    "X_train =  X_train.drop(['PPN','Personne'], axis=1)\n",
    "X_test_PPN_Personne  = X_test[['PPN','Personne']]\n",
    "X_test = X_test.drop(['PPN','Personne'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466180c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b94781",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test,Y_pred))\n",
    "print(metrics.classification_report(Y_test, Y_pred, digits=3))\n",
    "Matrice_Confusion(Y_test,Y_pred,\"KNN_13Classes\")\n",
    "visualisation_erreurs(df,X_test_PPN_Personne,Y_pred,Y_test,\"Eq_Sample_1000_13\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
