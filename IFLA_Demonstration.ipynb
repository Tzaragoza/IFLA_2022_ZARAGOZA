{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e517d8",
   "metadata": {},
   "source": [
    "<h1>IFLA WLIC - 2022</h1>\n",
    "<h2><span style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">From text to data inside bibliographic records. Entity recognition and entity linking of contributors and their roles from statements of responsibility</span></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1261bf",
   "metadata": {},
   "source": [
    "<h2>Pip installs - Necessary libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "080bb437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting texthero\n",
      "  Using cached texthero-1.1.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pandas>=1.0.2 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from texthero) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from texthero) (1.22.2)\n",
      "Requirement already satisfied: unidecode>=1.1.1 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from texthero) (1.2.0)\n",
      "Requirement already satisfied: tqdm>=4.3 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from texthero) (4.62.3)\n",
      "Collecting wordcloud>=1.5.0\n",
      "  Using cached wordcloud-1.8.1.tar.gz (220 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: matplotlib>=3.1.0 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from texthero) (3.4.3)\n",
      "Collecting spacy<3.0.0\n",
      "  Using cached spacy-2.3.7-cp39-cp39-win_amd64.whl (9.4 MB)\n",
      "Collecting gensim<4.0,>=3.6.0\n",
      "  Using cached gensim-3.8.3.tar.gz (23.4 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting plotly>=4.2.0\n",
      "  Using cached plotly-5.8.0-py2.py3-none-any.whl (15.2 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [17 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  running egg_info\n",
      "  writing gensim.egg-info\\PKG-INFO\n",
      "  writing dependency_links to gensim.egg-info\\dependency_links.txt\n",
      "  writing requirements to gensim.egg-info\\requires.txt\n",
      "  writing top-level names to gensim.egg-info\\top_level.txt\n",
      "  reading manifest file 'gensim.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no files found matching 'COPYING.LESSER'\n",
      "  warning: no files found matching 'ez_setup.py'\n",
      "  warning: no files found matching 'gensim\\models\\doc2vec_inner.c'\n",
      "  adding license file 'COPYING'\n",
      "  running build_ext\n",
      "  building 'gensim.models.word2vec_inner' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for gensim\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [59 lines of output]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from texthero) (0.24.2)\n",
      "Requirement already satisfied: nltk>=3.3 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from texthero) (3.6.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from gensim<4.0,>=3.6.0->texthero) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from gensim<4.0,>=3.6.0->texthero) (1.16.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from gensim<4.0,>=3.6.0->texthero) (5.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.1.0->texthero) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.0->texthero) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.0->texthero) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.1.0->texthero) (3.0.7)\n",
      "Requirement already satisfied: click in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from nltk>=3.3->texthero) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from nltk>=3.3->texthero) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from nltk>=3.3->texthero) (2021.8.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from pandas>=1.0.2->texthero) (2021.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from plotly>=4.2.0->texthero) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22->texthero) (2.2.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from spacy<3.0.0->texthero) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.0.0->texthero) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.0.0->texthero) (0.7.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from spacy<3.0.0->texthero) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.0.0->texthero) (3.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.0.0->texthero) (1.0.6)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from spacy<3.0.0->texthero) (7.4.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.0.0->texthero) (0.9.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\zaragoza\\anaconda3\\lib\\site-packages (from spacy<3.0.0->texthero) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.0.0->texthero) (2.27.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.0.0->texthero) (60.9.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.3->texthero) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zaragoza\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n",
      "Building wheels for collected packages: gensim, wordcloud\n",
      "  Building wheel for gensim (setup.py): started\n",
      "  Building wheel for gensim (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for gensim\n",
      "  Building wheel for wordcloud (setup.py): started\n",
      "  Building wheel for wordcloud (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for wordcloud\n",
      "Failed to build gensim wordcloud\n",
      "Installing collected packages: plotly, gensim, wordcloud, spacy, texthero\n",
      "  Running setup.py install for gensim: started\n",
      "  Running setup.py install for gensim: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  UPDATING build\\lib.win-amd64-3.9\\wordcloud/_version.py\n",
      "  set build\\lib.win-amd64-3.9\\wordcloud/_version.py to '1.8.1'\n",
      "  running build_ext\n",
      "  building 'wordcloud.query_integral_image' extension\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Local\\Temp\\pip-install-pfdd2k7k\\wordcloud_5dc359b2fd4744b4bf2df2adf56fdb72\\setup.py\", line 9, in <module>\n",
      "      setup(\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\__init__.py\", line 155, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\core.py\", line 148, in setup\n",
      "      return run_commands(dist)\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\core.py\", line 163, in run_commands\n",
      "      dist.run_commands()\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\dist.py\", line 967, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\dist.py\", line 986, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\wheel\\bdist_wheel.py\", line 299, in run\n",
      "      self.run_command('build')\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\dist.py\", line 986, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 135, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\dist.py\", line 986, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\command\\build_ext.py\", line 79, in run\n",
      "      _build_ext.run(self)\n",
      "    File \"C:\\Users\\zaragoza\\Anaconda3\\lib\\site-packages\\Cython\\Distutils\\old_build_ext.py\", line 186, in run\n",
      "      _build_ext.build_ext.run(self)\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 339, in run\n",
      "      self.build_extensions()\n",
      "    File \"C:\\Users\\zaragoza\\Anaconda3\\lib\\site-packages\\Cython\\Distutils\\old_build_ext.py\", line 195, in build_extensions\n",
      "      _build_ext.build_ext.build_extensions(self)\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 448, in build_extensions\n",
      "      self._build_extensions_serial()\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 473, in _build_extensions_serial\n",
      "      self.build_extension(ext)\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\command\\build_ext.py\", line 202, in build_extension\n",
      "      _build_ext.build_extension(self, ext)\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 528, in build_extension\n",
      "      objects = self.compiler.compile(sources,\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\_msvccompiler.py\", line 327, in compile\n",
      "      self.initialize()\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\_msvccompiler.py\", line 224, in initialize\n",
      "      vc_env = _get_vc_env(plat_spec)\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\msvc.py\", line 316, in msvc14_get_vc_env\n",
      "      return _msvc14_get_vc_env(plat_spec)\n",
      "    File \"C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\msvc.py\", line 270, in _msvc14_get_vc_env\n",
      "      raise distutils.errors.DistutilsPlatformError(\n",
      "  setuptools._distutils.errors.DistutilsPlatformError: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for wordcloud\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for gensim did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [19 lines of output]\n",
      "  running install\n",
      "  C:\\Users\\zaragoza\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  running egg_info\n",
      "  writing gensim.egg-info\\PKG-INFO\n",
      "  writing dependency_links to gensim.egg-info\\dependency_links.txt\n",
      "  writing requirements to gensim.egg-info\\requires.txt\n",
      "  writing top-level names to gensim.egg-info\\top_level.txt\n",
      "  reading manifest file 'gensim.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no files found matching 'COPYING.LESSER'\n",
      "  warning: no files found matching 'ez_setup.py'\n",
      "  warning: no files found matching 'gensim\\models\\doc2vec_inner.c'\n",
      "  adding license file 'COPYING'\n",
      "  running build_ext\n",
      "  building 'gensim.models.word2vec_inner' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "gensim\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n",
      "WARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\zaragoza\\Anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy\n",
    "#!pip install fuzzywuzzy\n",
    "#!pip install python-Levenshtein\n",
    "!pip install texthero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778789f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f0729",
   "metadata": {},
   "source": [
    "<h2>Imports - Necessary libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "20588c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formats\n",
    "import csv\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "\n",
    "# Functions\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Models\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "#Data pre-treatment\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff925c0",
   "metadata": {},
   "source": [
    "<h2>Demonstration</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72556a61",
   "metadata": {},
   "source": [
    "<h3>NER (Named Entity Recognition) Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26fab18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    écrit\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FONCT</span>\n",
       "</mark>\n",
       " par \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Zac Thompson\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " et \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Emily Horn\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    dessiné\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FONCT</span>\n",
       "</mark>\n",
       " par \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Alberto Albuquerque\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = spacy.load(\"./Model_NER_PERS_FONCT_10K\")\n",
    "doc = model(\"écrit par Zac Thompson et Emily Horn, dessiné par Alberto Albuquerque\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5047e17",
   "metadata": {},
   "source": [
    "<h3>Creating training sets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f0c2f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_match(name1,name2):\n",
    "    titreCivilite=0\n",
    "    score=0\n",
    "    name1=name1.replace(\"-\",\" \")\n",
    "    name1 = re.sub(r\"(.[.])()(.[.])\",r\"\\1 \\3\", name1)\n",
    "    name2=name2.replace(\"-\",\" \")\n",
    "    name_components_1 = name1.split(\" \")\n",
    "    name_components_2 = name2.split(\" \")\n",
    "    nameComponent1Match={}\n",
    "    nameComponent2Match={}\n",
    "    for index in range(len(name_components_1)):\n",
    "        name_component1=unidecode.unidecode(name_components_1[index]).lower()\n",
    "        ##TITLE##\n",
    "        if(index==0 and (name_component1 in [\"m.\",\"mme.\",\"dr.\"])):\n",
    "            titreCivilite=1\n",
    "            score+=1\n",
    "        ##ABREVIATIION##\n",
    "        elif(len(name_component1)<3 or (\".\" in name_component1)):\n",
    "            for index2 in range(len(name_components_2)):\n",
    "                name_component2=unidecode.unidecode(name_components_2[index2]).lower()\n",
    "                if(name_component2!=''):\n",
    "                    match=True\n",
    "                    for i in range(0,len(name_component1)):\n",
    "                        if(i<len(name_component2)and name_component1[i]!=name_component2[i] and name_component1[i]!=\".\"):\n",
    "                            match=False\n",
    "                    if(match):\n",
    "                        if(name_component1 not in nameComponent1Match.keys()):\n",
    "                            nameComponent1Match[name_component1]=1\n",
    "                        else:\n",
    "                            nameComponent1Match[name_component1]+=1\n",
    "                        if(name_component2 not in nameComponent2Match.keys()):\n",
    "                            nameComponent2Match[name_component2]=1\n",
    "                        else:\n",
    "                            nameComponent2Match[name_component2]+=1\n",
    "        ##Complete Name##\n",
    "        else:\n",
    "            for index2 in range(len(name_components_2)):\n",
    "                    name_component2=unidecode.unidecode(name_components_2[index2]).lower()\n",
    "                    ratio = fuzz.ratio(name_component1,name_component2)\n",
    "                    if(name_component2!=''):\n",
    "                        if(name_component1==name_component2 or (ratio>=80)):\n",
    "                            if(name_component1 not in nameComponent1Match.keys()):\n",
    "                                alignment=''\n",
    "                                for key in nameComponent1Match.keys():\n",
    "                                    if(fuzz.ratio(name_component1,key)>=80):\n",
    "                                        alignment=key\n",
    "                                if(alignment==''):\n",
    "                                    nameComponent1Match[name_component1]=1\n",
    "                                else:\n",
    "                                    nameComponent1Match[alignment]+=1\n",
    "                            else:\n",
    "                                nameComponent1Match[name_component1]+=1\n",
    "                            if(name_component2 not in nameComponent2Match.keys()):\n",
    "                                alignment=''\n",
    "                                for key in nameComponent2Match.keys():\n",
    "                                    if(fuzz.ratio(name_component2,key)>=80):\n",
    "                                        alignment=key\n",
    "                                if(alignment==''):\n",
    "                                    nameComponent2Match[name_component2]=1\n",
    "                                else:\n",
    "                                    nameComponent2Match[alignment]+=1\n",
    "                            else:\n",
    "                                nameComponent2Match[name_component2]+=1                        \n",
    "    for key in nameComponent1Match.keys():\n",
    "        score+=nameComponent1Match[key]    \n",
    "    \n",
    "    #print(\"Matching between \"+name1+\" and \"+name2+\" has a score of \"+str(score))\n",
    "    if(score==len(name_components_1) and (len(nameComponent2Match.keys())+titreCivilite)==(len(name_components_1))):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def create_trainingSet(load_name,save_name):\n",
    "    model = spacy.load(\"./Model_NER_PERS_FONCT_10K\")\n",
    "    rows=[]\n",
    "    file = open(\"./data/\"+load_name,mode='r',encoding=\"utf8\")\n",
    "    csvreader = csv.reader(file, delimiter=\",\")\n",
    "    header = next(csvreader)\n",
    "    dicoLabelAmt={}\n",
    "    #############For ever bibliographical record entry in the csv file#############\n",
    "    for row in csvreader:\n",
    "        ppn=row[1]\n",
    "        bibliographicalRecord = row[2]\n",
    "        accessPoints = row[3]\n",
    "        documentSupports = row[4]\n",
    "        ppnLinks = row[5]\n",
    "        textualRessources = row[6]\n",
    "        mentions = bibliographicalRecord.split(\" / \",1)\n",
    "        ###############################Creating table of supports#############################\n",
    "        supports = documentSupports.split(\",\")\n",
    "        ######################################################################################\n",
    "        ##########################Creating access points dictionary##########################\n",
    "        dictionnaireAccessPoints = {}\n",
    "        codes=[]\n",
    "        accessPoint = accessPoints.split(\" \")\n",
    "        name=\"\"\n",
    "        for token in accessPoint :\n",
    "            if(token.isnumeric() and len(name)==0):\n",
    "                codes.append(token)\n",
    "            elif(token.isnumeric()):\n",
    "                #Checking the person is associate to only a single UNIMARC code function to avoi ambiguity\n",
    "                if (len(name)>0 and len(codes)==1):\n",
    "                    dictionnaireAccessPoints[name]=codes\n",
    "                codes=[]\n",
    "                name=\"\"\n",
    "                codes.append(token)\n",
    "            elif(len(name)==0):\n",
    "                name+=token\n",
    "            else :\n",
    "                name+=\" \"\n",
    "                name+=token\n",
    "        #Checking the person is associate to only a single UNIMARC code function to avoi ambiguity\n",
    "        if (len(name)>0 and len(codes)==1):\n",
    "            dictionnaireAccessPoints[name]=codes\n",
    "        ########################################################################################\n",
    "        ###################################Feature 'Thesis'#####################################\n",
    "        isThesis = False\n",
    "        for link in ppnLinks.split(','):\n",
    "            if(link==\"027253139\"):\n",
    "                isThesis=True\n",
    "        for ressource in textualRessources.split(','):\n",
    "            if(('m' in ressource[4:8])or('v' in ressource[4:8])):\n",
    "                isThesis=True\n",
    "        ########################################################################################\n",
    "        ####################################Extracting LABEL####################################\n",
    "        if(len(mentions)>=2):\n",
    "            ###Position of the mention###\n",
    "            position=0\n",
    "            ###Iterating through the mention of responsibilities###\n",
    "            for mention in mentions[1].split(\" ; \"):\n",
    "                dictPersonFunct={}\n",
    "                doc = model(mention)\n",
    "                ###Pairing function keywords to their appropriate persons###\n",
    "                persons=[]\n",
    "                keywords=[]\n",
    "                currentFunct=''\n",
    "                switch=0\n",
    "                for ent in doc.ents:\n",
    "                    if(currentFunct==\"\"):\n",
    "                        currentFunct=ent.label_\n",
    "                    if(currentFunct!=ent.label_):\n",
    "                        currentFunct=ent.label_\n",
    "                        switch+=1\n",
    "                    if(switch%2==0 and switch>0):\n",
    "                        for person in persons:\n",
    "                            if(person in dictPersonFunct.keys()):\n",
    "                                dictPersonFunct[person].append(keywords)\n",
    "                            else:\n",
    "                                dictPersonFunct[person]=keywords\n",
    "                        persons=[]\n",
    "                        keywords=[]\n",
    "                    if(ent.label_== \"PER\"):\n",
    "                        persons.append(ent)\n",
    "                    if(ent.label_ == \"FONCT\"):\n",
    "                        keywords.append(ent)\n",
    "                for person in persons:\n",
    "                    if(person in dictPersonFunct.keys()):\n",
    "                        dictPersonFunct[person].append(keywords)\n",
    "                    else:\n",
    "                        dictPersonFunct[person]=keywords\n",
    "                ############################################################\n",
    "                for person in dictPersonFunct.keys():\n",
    "                    for key in dictionnaireAccessPoints.keys():\n",
    "                        if(name_match(person.text, key)):\n",
    "                            ###Counting amount of entries per function###\n",
    "                            if(dictionnaireAccessPoints[key][0] in dicoLabelAmt.keys()):\n",
    "                                dicoLabelAmt[dictionnaireAccessPoints[key][0]]+=1\n",
    "                            else:\n",
    "                                dicoLabelAmt[dictionnaireAccessPoints[key][0]]=1\n",
    "                            ############################################\n",
    "                            rows.append([ppn,person.text,dictPersonFunct[person],position,supports,isThesis,dictionnaireAccessPoints[key][0]])\n",
    "                ###incrementing position of the mention###\n",
    "                position+=1\n",
    "    ########################################################################################\n",
    "    ##################################Writing training dataset##############################\n",
    "    file2 = open(\"./data/TRAINING_PPN_PERSON_Keywords_Position_DocSupport_Thesis\"+save_name+\".csv\",mode='w',encoding=\"utf8\",newline='')\n",
    "    writer = csv.writer(file2)\n",
    "    writer.writerow([\"PPN\",\"Person\",\"Keywords\",\"Position\",\"Document Support\",\"Thesis\",\"Label\"])\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "    print(\"There are \"+str(len(rows))+\" entries\")\n",
    "    print(\"Final count of training entries per function :\")\n",
    "    for key in dicoLabelAmt.keys():\n",
    "        print(\"Function \"+str(key)+\" has \"+str(dicoLabelAmt[key])+\" entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f3b484f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching between these should be accepted\n",
      " \n",
      "F. Nietzsche and Friedrich Nietzsche are matching: True\n",
      "Token_Set_Ratio is :90\n",
      "Alberto Albuquerque and Alberto Jiménez Albuquerque are matching: True\n",
      "Token_Set_Ratio is :100\n",
      "M. Zaragoza and Thomas Zaragoza are matching: True\n",
      "Token_Set_Ratio is :89\n",
      "Mme. Zaragoza and Zaragoza Thomas are matching: True\n",
      "Token_Set_Ratio is :80\n",
      "Thomâs Zargoza and Thomas Zaragoza are matching: True\n",
      "Token_Set_Ratio is :93\n",
      "Mircel Frontignan and Marcel Frontagnan are matching: True\n",
      "Token_Set_Ratio is :88\n",
      "J-C Van Damme and Jean-Claude Van Damme are matching: True\n",
      "Token_Set_Ratio is :82\n",
      "Th Maximilien and Théophile Maximilien are matching: True\n",
      "Token_Set_Ratio is :87\n",
      "J.L. Istin and Jean-Luc Istin are matching: True\n",
      "Token_Set_Ratio is :78\n",
      " \n",
      "========================================\n",
      " \n",
      "Matching between these should be refused\n",
      " \n",
      "G. Nietzsche and Friedrich Nietzsche are matching : False\n",
      "Frank Nietzsche and Friedrich Nietzsche are matching : False\n",
      "Z. Zaragoza and Thomas Zaragoza are matching : False\n",
      "Marcel Frontignan and Marco Frontignan are matching : False\n"
     ]
    }
   ],
   "source": [
    "true_name1=[\"F. Nietzsche\",\"Alberto Albuquerque\",\"M. Zaragoza\",\"Mme. Zaragoza\",\"Thomâs Zargoza\",\"Mircel Frontignan\",\"J-C Van Damme\",\"Th Maximilien\",\"J.L. Istin\"]\n",
    "true_name2=[\"Friedrich Nietzsche\",\"Alberto Jiménez Albuquerque\",\"Thomas Zaragoza\",\"Zaragoza Thomas\",\"Thomas Zaragoza\",\"Marcel Frontagnan\",\"Jean-Claude Van Damme\",\"Théophile Maximilien\",\"Jean-Luc Istin\"]\n",
    "false_name1=[\"G. Nietzsche\",\"Frank Nietzsche\",\"Z. Zaragoza\",\"Marcel Frontignan\"]\n",
    "false_name2=[\"Friedrich Nietzsche\",\"Friedrich Nietzsche\",\"Thomas Zaragoza\",\"Marco Frontignan\"]\n",
    "print(\"Matching between these should be accepted\")\n",
    "print(\" \")\n",
    "for x in range(len(true_name1)):\n",
    "    print(true_name1[x]+\" and \"+true_name2[x]+\" are matching: \"+str(name_match(true_name1[x],true_name2[x])))\n",
    "    Token_Set_Ratio = fuzz.token_set_ratio(true_name1[x],true_name2[x])\n",
    "    print(\"Token_Set_Ratio is :\"+str(Token_Set_Ratio))\n",
    "print(\" \")\n",
    "print(\"========================================\")\n",
    "print(\" \")\n",
    "print(\"Matching between these should be refused\")\n",
    "print(\" \")\n",
    "for x in range(len(false_name1)):\n",
    "    print(false_name1[x]+\" and \"+false_name2[x]+\" are matching : \"+str(name_match(false_name1[x],false_name2[x])))\n",
    "    Token_Set_Ratio = fuzz.token_set_ratio(false_name1[x],false_name2[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "52649c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 entries\n",
      "Final count of training entries per function :\n",
      "Function 070 has 2 entries.\n",
      "Function 440 has 1 entries.\n",
      "Function 410 has 1 entries.\n",
      "Function 730 has 1 entries.\n",
      "Function 005 has 3 entries.\n"
     ]
    }
   ],
   "source": [
    "create_trainingSet(\"ID_PPN_Mentions_AP_DocSup_PPNLink_TextualRessources_TEST.csv\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ef8ed",
   "metadata": {},
   "source": [
    "<h2>Loading and encoding training set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1fec92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodage_données(dataframe,featuresBinaires, featuresOrdinaux, featuresNominaux, featuresTextes):\n",
    "    encodedDataframe = dataframe.copy()\n",
    "    ###Relocating target##\n",
    "    labelColumn = encodedDataframe[\"Label\"] \n",
    "    del encodedDataframe[\"Label\"]\n",
    "    ######################\n",
    "    clean_pipeline = [#preprocessing.fillna,\n",
    "                   preprocessing.lowercase,\n",
    "                   preprocessing.remove_whitespace,\n",
    "                   preprocessing.remove_diacritics\n",
    "                   #preprocessing.remove_brackets\n",
    "                  ]\n",
    "    ######################\n",
    "    binaryEncoders=[]\n",
    "    ordinalEncoders=[]\n",
    "    nominalEncoders=[]\n",
    "    textEncoders=[]\n",
    "    ###Categorical Features###\n",
    "    for binaryFeature in featuresBinaires:\n",
    "        print(\"Encoding Binary Feature : \"+str(binaryFeature))\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(encodedDataframe[binaryFeature])\n",
    "        encodedDataframe[binaryFeature]=label_encoder.transform(dataframe[binaryFeature])\n",
    "        binaryEncoders.append(label_encoder)\n",
    "    for ordinalFeature in featuresOrdinaux:\n",
    "        pass\n",
    "        #IMPLEMENT\n",
    "    for nominalFeature in featuresNominaux:\n",
    "        print(\"Encoding Nominal Feature : \"+str(nominalFeature))\n",
    "        encodedDataframe = pd.get_dummies(encodedDataframe, prefix = [nominalFeature], columns = [nominalFeature])\n",
    "        nominalEncoders.append(nominalFeature)\n",
    "    ###Text Features###\n",
    "    for textFeature in featuresTextes:\n",
    "        print(\"Encoding Text Feature : \"+str(textFeature))\n",
    "        #######CLEAN UP#########\n",
    "        encodedDataframe[textFeature] = [n.replace('[','') for n in encodedDataframe[textFeature]]\n",
    "        encodedDataframe[textFeature] = [n.replace(']','') for n in encodedDataframe[textFeature]]\n",
    "        encodedDataframe[textFeature] = hero.clean(encodedDataframe[textFeature], clean_pipeline)\n",
    "        ########################\n",
    "        ###Creating TF-IDF df###\n",
    "        encodedDataframe[textFeature]=hero.tfidf(encodedDataframe[textFeature], max_features=3000)\n",
    "        allwordsAmt=len(encodedDataframe[textFeature][0])\n",
    "        lsts=list(encodedDataframe[textFeature])\n",
    "        column_names=[]\n",
    "        for i in range(0,allwordsAmt):\n",
    "            column_names.append(str(textFeature)+\"_\"+str(i))\n",
    "        dataframeTmp = pd.DataFrame(lsts, columns =column_names, dtype = float)\n",
    "        ########################\n",
    "        del encodedDataframe[textFeature]\n",
    "        encodedDataframe = pd.concat([encodedDataframe.reset_index(drop=True),dataframeTmp.reset_index(drop=True)], axis=1)\n",
    "    ###Relocating target###\n",
    "    #encodedDataframe[\"Label\"]=labelColumn\n",
    "    #######################\n",
    "    return encodedDataframe,labelColumn,binaryEncoders, ordinalEncoders, nominalEncoders, textEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7aa387bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/TRAINING_Classes_13_Sample_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "466180c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PPN</th>\n",
       "      <th>Personne</th>\n",
       "      <th>Position</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Document support</th>\n",
       "      <th>Thesis</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262419734</td>\n",
       "      <td>Béatrix Lot</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['txt', 'sti']</td>\n",
       "      <td>False</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262419610</td>\n",
       "      <td>Amadou Amal</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['txt']</td>\n",
       "      <td>False</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262419572</td>\n",
       "      <td>Séverine Vidal</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['txt']</td>\n",
       "      <td>False</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262419467</td>\n",
       "      <td>Fabrice Amedeo</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['txt']</td>\n",
       "      <td>False</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262419378</td>\n",
       "      <td>Bruno d'Halluin</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>['txt']</td>\n",
       "      <td>False</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12995</th>\n",
       "      <td>249884380</td>\n",
       "      <td>Anna Gavalda</td>\n",
       "      <td>1</td>\n",
       "      <td>[aut., adapté]</td>\n",
       "      <td>['tdi']</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12996</th>\n",
       "      <td>24987900X</td>\n",
       "      <td>Maurice Blanchot</td>\n",
       "      <td>3</td>\n",
       "      <td>[aut.]</td>\n",
       "      <td>['tdi']</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12997</th>\n",
       "      <td>24987900X</td>\n",
       "      <td>Edgar Allan Poe</td>\n",
       "      <td>3</td>\n",
       "      <td>[aut.]</td>\n",
       "      <td>['tdi']</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12998</th>\n",
       "      <td>249876671</td>\n",
       "      <td>Joseph Conrad</td>\n",
       "      <td>1</td>\n",
       "      <td>[aut.]</td>\n",
       "      <td>['tdi']</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12999</th>\n",
       "      <td>249876671</td>\n",
       "      <td>Heiner Müller</td>\n",
       "      <td>1</td>\n",
       "      <td>[aut.]</td>\n",
       "      <td>['tdi']</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PPN          Personne  Position        Keywords Document support  \\\n",
       "0      262419734       Béatrix Lot         0              []   ['txt', 'sti']   \n",
       "1      262419610       Amadou Amal         0              []          ['txt']   \n",
       "2      262419572    Séverine Vidal         0              []          ['txt']   \n",
       "3      262419467    Fabrice Amedeo         0              []          ['txt']   \n",
       "4      262419378   Bruno d'Halluin         0              []          ['txt']   \n",
       "...          ...               ...       ...             ...              ...   \n",
       "12995  249884380      Anna Gavalda         1  [aut., adapté]          ['tdi']   \n",
       "12996  24987900X  Maurice Blanchot         3          [aut.]          ['tdi']   \n",
       "12997  24987900X   Edgar Allan Poe         3          [aut.]          ['tdi']   \n",
       "12998  249876671     Joseph Conrad         1          [aut.]          ['tdi']   \n",
       "12999  249876671     Heiner Müller         1          [aut.]          ['tdi']   \n",
       "\n",
       "       Thesis  Label  \n",
       "0       False     70  \n",
       "1       False     70  \n",
       "2       False     70  \n",
       "3       False     70  \n",
       "4       False     70  \n",
       "...       ...    ...  \n",
       "12995   False    100  \n",
       "12996   False    100  \n",
       "12997   False    100  \n",
       "12998   False    100  \n",
       "12999   False    100  \n",
       "\n",
       "[13000 rows x 7 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5dfca11b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.preprocessing' has no attribute 'lowercase'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [105]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encodedDataframe,targetDataframe,binaryEncoders, ordinalEncoders, nominalEncoders, textEncoders \u001b[38;5;241m=\u001b[39m \u001b[43mencodage_données\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThesis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDocument support\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKeywords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [102]\u001b[0m, in \u001b[0;36mencodage_données\u001b[1;34m(dataframe, featuresBinaires, featuresOrdinaux, featuresNominaux, featuresTextes)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m encodedDataframe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m######################\u001b[39;00m\n\u001b[0;32m      7\u001b[0m clean_pipeline \u001b[38;5;241m=\u001b[39m [\u001b[38;5;66;03m#preprocessing.fillna,\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m                \u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlowercase\u001b[49m,\n\u001b[0;32m      9\u001b[0m                preprocessing\u001b[38;5;241m.\u001b[39mremove_whitespace,\n\u001b[0;32m     10\u001b[0m                preprocessing\u001b[38;5;241m.\u001b[39mremove_diacritics\n\u001b[0;32m     11\u001b[0m                \u001b[38;5;66;03m#preprocessing.remove_brackets\u001b[39;00m\n\u001b[0;32m     12\u001b[0m               ]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m######################\u001b[39;00m\n\u001b[0;32m     14\u001b[0m binaryEncoders\u001b[38;5;241m=\u001b[39m[]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn.preprocessing' has no attribute 'lowercase'"
     ]
    }
   ],
   "source": [
    "encodedDataframe,targetDataframe,binaryEncoders, ordinalEncoders, nominalEncoders, textEncoders = encodage_données(df,[\"Thesis\"],[],[],[\"Document support\",\"Keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b94781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
